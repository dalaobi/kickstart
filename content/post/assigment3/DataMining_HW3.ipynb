{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "import re\n",
    "import math\n",
    "from random import choice\n",
    "import time\n",
    "import _pickle as cpickle\n",
    "import os\n",
    "\n",
    "def get_data(x_file, y_file):\n",
    "    x_file_obj = open(x_file, 'r', encoding=\"utf8\")\n",
    "    x = []\n",
    "    for line in x_file_obj:\n",
    "        x.append(line)\n",
    "    y = np.loadtxt(y_file)\n",
    "    return x, y\n",
    "\n",
    "def get_word_counts(words):\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "    return word_counts\n",
    "        \n",
    "def train_model(x, y, modi, smooth=False):\n",
    "    vocab_dict = {}\n",
    "    word_count_labels = {}\n",
    "    theta_words = {}\n",
    "    len_of_examples = {}\n",
    "    labels_count = {}\n",
    "    log_labels_prior = {}\n",
    "    if(modi):\n",
    "        file_train_name = \"objs/train_obj_modi\"\n",
    "    else:\n",
    "        file_train_name = \"objs/train_obj\"\n",
    "    if os.path.exists(file_train_name):\n",
    "        fileObj = open(file_train_name, 'rb')\n",
    "        arr = cpickle.load(fileObj)\n",
    "        log_labels_prior = arr[0] \n",
    "        theta_words = arr[1]\n",
    "        vocab_dict = arr[2]\n",
    "        labels_count = arr[3]\n",
    "        return log_labels_prior, theta_words, vocab_dict, labels_count\n",
    "    \n",
    "    N1 = len(y)\n",
    "    for i in range(1,11):\n",
    "        if i == 5 or i == 6: continue\n",
    "        word_count_labels[i] = {}\n",
    "        theta_words[i] = {}\n",
    "        len_of_examples[i] = 0\n",
    "\n",
    "    i = 0\n",
    "    for line in x:\n",
    "#         tokens = nltk.word_tokenize(line)\n",
    "        line = line.lower()\n",
    "        tokens = re.split(\"\\W+\", line)\n",
    "        word_count = get_word_counts(tokens)\n",
    "        for word, count in word_count.items():\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word] = 0\n",
    "            if word not in word_count_labels[y[i]]:\n",
    "                word_count_labels[y[i]][word] = 0.0\n",
    "            word_count_labels[y[i]][word] += count\n",
    "        if y[i] not in labels_count:\n",
    "            labels_count[y[i]] = 0.0\n",
    "        labels_count[y[i]] += 1\n",
    "        len_of_examples[y[i]] += len(tokens)\n",
    "        i+=1\n",
    "        \n",
    "    for j in range(1,11):\n",
    "        if j ==5 or j == 6: continue\n",
    "        log_labels_prior[j] = math.log(labels_count[j] / N1)\n",
    "        \n",
    "    for j in range(1,11):\n",
    "        if j == 5 or j == 6: continue\n",
    "        for word in vocab_dict:\n",
    "            if smooth:\n",
    "                temp = (int(word_count_labels[j].get(word,0.0)) + 1) / (int(len_of_examples[j]) + int(len(vocab_dict)))\n",
    "            else:\n",
    "                temp = (int(word_count_labels[j].get(word,0.0))) / (int(len_of_examples[j]))\n",
    "            theta_words[j][word] = temp   \n",
    "    fileObj = open(file_train_name,'wb')    \n",
    "    arr = [log_labels_prior, theta_words, vocab_dict, labels_count]     \n",
    "    cpickle.dump(arr, fileObj)\n",
    "    return log_labels_prior, theta_words, vocab_dict, labels_count\n",
    "\n",
    "\"\"\" accuracy train = 78.15, test = 40.33% \"\"\"    \n",
    "def naive_bayes_classifier(x, log_labels_prior, theta_words, vocab_dict, modi, train):\n",
    "    if(modi):\n",
    "        if(train):\n",
    "            file_nv_c_name = \"objs/nv_c_res_modi_train\"\n",
    "        else:\n",
    "            file_nv_c_name = \"objs/nv_c_res_modi_test\"\n",
    "    else:\n",
    "        if(train):\n",
    "            file_nv_c_name = \"objs/nv_c_res_train\"\n",
    "        else:\n",
    "            file_nv_c_name = \"objs/nv_c_res_test\"\n",
    "    if os.path.exists(file_nv_c_name):\n",
    "        fileObj = open(file_nv_c_name, 'rb')\n",
    "        result = cpickle.load(fileObj)\n",
    "        return result\n",
    "    \n",
    "    result = []\n",
    "    for line in x:\n",
    "        #tokens = nltk.word_tokenize(line)\n",
    "        #line = line.lower()\n",
    "        tokens = re.split(\"\\W+\", line)\n",
    "        class_label_score = {1:0, 2:0, 3:0, 4:0, 7:0, 8:0, 9:0, 10:0}\n",
    "        word_count = get_word_counts(tokens)\n",
    "        for word, count in word_count.items():\n",
    "            if word not in vocab_dict: continue\n",
    "            for i in range(1,11):\n",
    "                if i== 5 or i == 6: continue\n",
    "                class_label_score[i] += (math.log(theta_words[i][word]))\n",
    "            \n",
    "        for i in range(1,11):\n",
    "            if i == 5 or i == 6:continue\n",
    "            class_label_score[i] += float((log_labels_prior[i]))\n",
    "        #print(class_label_score)\n",
    "        result.append(max(class_label_score, key = class_label_score.get))\n",
    "        \n",
    "    fileObj = open(file_nv_c_name,'wb')    \n",
    "    cpickle.dump(result, fileObj)    \n",
    "    return result\n",
    "\n",
    "def prediction(x, log_labels_prior, theta_words, vocab_dict, labels_count, modi, train):\n",
    "    result = naive_bayes_classifier(x, log_labels_prior, theta_words, vocab_dict, modi, train)\n",
    "    return np.array(result)\n",
    "\n",
    "def find_accuracy(y, result):\n",
    "    N = len(y)\n",
    "    sum = 0\n",
    "    for i in range(0,N):\n",
    "        if y[i] == result[i]:\n",
    "            sum += 1\n",
    "    return sum/N\n",
    "    \n",
    "def create_conf_matrix(expected, predicted):\n",
    "    conf_mat = np.zeros((11,11), dtype = int)\n",
    "    N = len(expected)\n",
    "    for i in range(0,N):\n",
    "        if int(expected[i]) == 5 or int(expected[i]) == 6: continue\n",
    "        conf_mat[int(expected[i])][int(predicted[i])] +=1\n",
    "    \n",
    "    conf_mat = np.delete(conf_mat, 0, 0)\n",
    "    conf_mat = np.delete(conf_mat, 4, 0)\n",
    "    conf_mat = np.delete(conf_mat, 4, 0)\n",
    "    conf_mat = np.delete(conf_mat, 0, 1)\n",
    "    conf_mat = np.delete(conf_mat, 4, 1)\n",
    "    conf_mat = np.delete(conf_mat, 4, 1)\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training set is :0.6953\n",
      "The accuracy of test set is :0.2954\n",
      "--- 0.33498287200927734 seconds ---\n"
     ]
    }
   ],
   "source": [
    "x_train_file = \"imdb_rev/imdb_train_text_modified.txt\"\n",
    "y_train_file = \"imdb_rev/imdb_train_labels.txt\"\n",
    "x_test_file = \"imdb_rev/imdb_test_text_modified.txt\"\n",
    "y_test_file = \"imdb_rev/imdb_test_labels.txt\"\n",
    "start_time = time.time()\n",
    "X_train, Y_train = get_data(x_train_file, y_train_file)\n",
    "X_test, Y_test = get_data(x_test_file, y_test_file)\n",
    "log_labels_prior, theta_words, vocab_dict, labels_count = train_model(X_train, Y_train, 1)\n",
    "\"\"\"\n",
    "classifier:\n",
    "1.-> random classifier\n",
    "2. -> majority classifier\n",
    "3. -> naive bayes classifier\n",
    "\"\"\"\n",
    "result = prediction(X_train, log_labels_prior,theta_words, vocab_dict, labels_count, 1, 1) #classifier, modi, train\n",
    "accuracy = find_accuracy(Y_train, result)\n",
    "print((\"The accuracy of training set is :{0:.4f}\".format(accuracy)))\n",
    "test_result = prediction(X_test, log_labels_prior,theta_words, vocab_dict, labels_count, 1, 1) #classifier, modi, train\n",
    "test_accuracy = find_accuracy(Y_test, test_result)\n",
    "print((\"The accuracy of test set is :{0:.4f}\".format(test_accuracy)))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5000  5001  5002 ... 24997 24998 24999]\n",
      "[   0    1    2 ... 4997 4998 4999]\n",
      "The accuracy of current cross validation is :0.6922\n",
      "[    0     1     2 ... 24997 24998 24999]\n",
      "[5000 5001 5002 ... 9997 9998 9999]\n",
      "The accuracy of current cross validation is :0.2786\n",
      "[    0     1     2 ... 24997 24998 24999]\n",
      "[10000 10001 10002 ... 14997 14998 14999]\n",
      "The accuracy of current cross validation is :0.1582\n",
      "[    0     1     2 ... 24997 24998 24999]\n",
      "[15000 15001 15002 ... 19997 19998 19999]\n",
      "The accuracy of current cross validation is :0.0340\n",
      "[    0     1     2 ... 19997 19998 19999]\n",
      "[20000 20001 20002 ... 24997 24998 24999]\n",
      "The accuracy of current cross validation is :0.0354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "x_train_file = \"imdb_rev/imdb_train_text_modified.txt\"\n",
    "y_train_file = \"imdb_rev/imdb_train_labels.txt\"\n",
    "x_test_file = \"imdb_rev/imdb_test_text_modified.txt\"\n",
    "y_test_file = \"imdb_rev/imdb_test_labels.txt\"\n",
    "start_time = time.time()\n",
    "X_train, Y_train = get_data(x_train_file, y_train_file)\n",
    "X_test, Y_test = get_data(x_test_file, y_test_file)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X_train)\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "    print(train_index)\n",
    "    print(test_index)\n",
    "    X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]\n",
    "    Y_train_cv, Y_test_cv = Y_train[train_index], Y_train[test_index]\n",
    "    log_labels_prior, theta_words, vocab_dict, labels_count = train_model(X_train_cv, Y_train_cv, 1)\n",
    "    result = prediction(X_test_cv, log_labels_prior, theta_words, vocab_dict, labels_count, 1, 1) #classifier, modi, train\n",
    "    accuracy = find_accuracy(Y_test_cv, result)\n",
    "    print(\"The accuracy of current cross validation is :{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "The accuracy of test set without smooth is :0.2954\n",
      "--------------------------------------------------\n",
      "The accuracy of test set with smooth is :0.3955\n"
     ]
    }
   ],
   "source": [
    "x_train_file = \"imdb_rev/imdb_train_text_modified.txt\"\n",
    "y_train_file = \"imdb_rev/imdb_train_labels.txt\"\n",
    "x_test_file = \"imdb_rev/imdb_test_text_modified.txt\"\n",
    "y_test_file = \"imdb_rev/imdb_test_labels.txt\"\n",
    "start_time = time.time()\n",
    "X_train, Y_train = get_data(x_train_file, y_train_file)\n",
    "X_test, Y_test = get_data(x_test_file, y_test_file)\n",
    "print('-----'*10)\n",
    "log_labels_prior1, theta_words1, vocab_dict1, labels_count1 = train_model(X_train, Y_train, 1, False)\n",
    "result = prediction(X_test, log_labels_prior1, theta_words1, vocab_dict1, labels_count1, 1, 1) #classifier, modi, train\n",
    "accuracy = find_accuracy(Y_test, result)\n",
    "print((\"The accuracy of test set without smooth is :{0:.4f}\".format(accuracy)))\n",
    "print('-----'*10)\n",
    "log_labels_prior2, theta_words2, vocab_dict2, labels_count2 = train_model(X_train, Y_train, 0, True)\n",
    "test_result = prediction(X_test, log_labels_prior2, theta_words2, vocab_dict2, labels_count2, 0, 1) #classifier, modi, train\n",
    "test_accuracy = find_accuracy(Y_test, test_result)\n",
    "print((\"The accuracy of test set with smooth is :{0:.4f}\".format(test_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
